{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\" ><font size=\"7\" >Creation of the PePrMInt Dataset</font></p>\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: Thibault Tubiana  \n",
    "**Version**: 1.0  \n",
    "**Last change**: Major refactoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PREPARATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECALCULATION = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Depandancies\n",
    " --> See `00-SETYP.ipynb` for depandancies and configuration\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    " --> Please run `01-download_files.ipynb` before running this notebook, it countains all the code to download the required files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thibault/miniconda3/envs/peprmint/lib/python3.7/site-packages/tqdm/std.py:699: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "#Pandas configuration\n",
    "pd.options.mode.chained_assignment = (\n",
    "    None  # default='warn', remove pandas warning when adding a new column\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%config InlineBackend.figure_format ='svg' #better quality figure figure\n",
    "\n",
    "from tqdm.notebook import tnrange, tqdm\n",
    "tqdm.pandas()  # activate tqdm progressbar for pandas apply\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all global variables and setings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./00-SETUP.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Choose here** if a **full recomputation** is needed or if loading a checkpoint after the initial structural dataset computation is enough.  \n",
    "  This is usefull if you want "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd91037d7d744a62bcace96c05693a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButton(value=False, description='Recalculation ?', icon='cogs', tooltip='Click for recalculation')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recalculation_widget = widgets.ToggleButton(\n",
    "    value=RECALCULATION,\n",
    "    description='Recalculation ?',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click for recalculation',\n",
    "    icon='cogs' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "display(recalculation_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciating the builder object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pepr2ds.builder.Builder' from '/Users/thibault/OneDrive - University of Bergen/projects/peprmint/dev/pepermintdataset/pepr2ds/builder/Builder.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import builder.Builder as builderEngine\n",
    "# importlib.reload(builderEngine)\n",
    "\n",
    "# builder = builderEngine.Builder(SETUP, recalculate = recalculation_widget.value, update=False, notebook = True, core=1)\n",
    "\n",
    "import pepr2ds.builder.Builder as builderEngine\n",
    "importlib.reload(builderEngine)\n",
    "builder = builderEngine.Builder(SETUP, recalculate = recalculation_widget.value, update=False, notebook = True, core=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **STRUCTURAL DATASET**\n",
    "## PDB Parsing and computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create a dataset based on all PDBs. Here's a quick description\n",
    "1. For each CATH domains, all the PDBS (previously downloaded with `01-download_cathpdb.ipynb`) are **loaded one by one** and **cleaned**.\n",
    "2. The PDBs will be **parsed with biopandas** to be transformed in a **DataFrame**. It will looks like this:  \n",
    "    ![Example biopandas (need internet connectivity)](ressources/biopandas.png) \n",
    "3. The **secondary structure** will be calculated with [**DSSP**  ](https://swift.cmbi.umcn.nl/gv/dssp/)  \n",
    "    ![secondary_structures](ressources/secondaryStructures.png)\n",
    "4. The **Accessible Surface Area** is then computed with [**FREESASA**](http://freesasa.github.io/python/functions.html) (and DSSP..)  \n",
    "    <div><img src=\"ressources/surface-diagram.png\" width=\"400\"></div>\n",
    "5. **Proteins block** are calculated with [**PBxplore**](https://github.com/pierrepo/PBxplore) (check https://github.com/pierrepo/PBxplore)  \n",
    "    <div><img src=\"ressources/PBs.jpg\" width=\"400\"></div>\n",
    "6. Then everything will be merged and saved in the `WORKDIR/dataset`  \n",
    "    ![secondary_structures](ressources/floppy.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.structure.clean_all_pdbs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Reading checkpoint 1\n"
     ]
    }
   ],
   "source": [
    "DATASET = builder.structure.build_structural_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cathpdb', 'alfafold'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET.data_type.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing protrusion models\n",
    "Based on atom properties of all PDBs (which are in a dataset datastructure), we will compute **convexhull**, **vertices**, **protrusions**, **co-insertables** according Fuglebakk *et al.*, PONe, 2018: https://doi.org/10.1371/journal.pcbi.1006325  \n",
    "![](ressources/protrusions.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Conputing protrusions .. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33dfaea3259242618544613b27c4b673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9550.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DATASET = builder.structure.add_protrusions(DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding structural cluster information\n",
    "\n",
    "To remove redundancy on our dataset (or filter based on redundancy later on), we need cluster informations. For that, we will use <img src=\"ressources/cathlogo.png\" width=\"40\"> clusters\n",
    "\n",
    "In CATH, we can find info about the the sequence cluster.  \n",
    "Note that there is 4 clusters:\n",
    " - S35 : Structure with 35% of sequence idendity\n",
    " - S60 : Structure with 65% of sequence idendity\n",
    " - S95 : Structure with 95% of sequence idendity\n",
    " - S100 : Structure with 100% of sequence idendity\n",
    "\n",
    "But the cluster number is hierarchical actually.  \n",
    "```\n",
    "S35\n",
    " |_S60\n",
    "    |_S95\n",
    "       |_S100\n",
    "```\n",
    "So we need to make a transformation to keep the cluster info. \n",
    "For example, a sequence that is in the cluster 2 at 35%, cluster 4 at 60%, cluster 3 at 95% and cluster 1 at 100% will be `2.4.3.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pepr2ds.builder.Builder' from '/Users/thibault/OneDrive - University of Bergen/projects/peprmint/dev/pepermintdataset/pepr2ds/builder/Builder.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pepr2ds.builder.Builder as builderEngine\n",
    "importlib.reload(builderEngine)\n",
    "builder = builderEngine.Builder(SETUP, recalculate = recalculation_widget.value, update=False, notebook = True, core=1)\n",
    "\n",
    "DATASET = builder.structure.add_structural_cluster_info(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **WORKING WITH SEQUENCES**  \n",
    "Now that we are done with pdb files, with can work on sequences and other external databases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding uniprot information (origin and uniprot_id)\n",
    "\n",
    "From the PDBid, we will fetch **origin** and **uniprot_id** (like `ASAP1_HUMAN`) from the **uniprot_acc** (`Q9ULH1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1c99f23a1b4b319c8280a72278b513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9030128.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DATASET = builder.sequence.add_uniprotId_Origin(DATASET)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting to add sequences in DATASET\n",
    "\n",
    "We can start to add sequences in our dataset from **PROSITE**. \n",
    "1. We take the multiple alignment from **prosite** and get all sequences.\n",
    "2. for now we keep only sequences that are already in the DATASET (sequences that have a PDB structure)\n",
    "3. we match the residue number of each residues in the PDB, with the residue position in the alignment\n",
    "4. we add all those new data in the DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Matching between the structure and the sequence aligned...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362c5232f7ea47ac8cc55f513a6a807d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f32c95939c460aa9d42f718783dafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=451.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e97c80970ae4741ad7151dec83bb928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=187.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f299dff9b67403b83dfb418d1acf1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=333.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99832f38e60d470e9bc101042496bdd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=755.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc82df85d760454f9148910714b99390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1340.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3162c40471484a06827b4af4d07ba58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=186.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da26e7c6406640e3ab06622f83f185c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2245.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATASET = builder.sequence.match_residue_number_with_alignment_position(DATASET)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding sequence without structures  \n",
    "\n",
    "Of course all sequences doesn't have a structure, but they can be quite usefull for certain statistics (amino acid composition, conservation..), so we add them in our dataset even if they don't have a structure üôÇ.  \n",
    "For every amino acid of every sequences, we will consider the residue as a `CA` atom, to match the current dataframe structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5c9480a80d402d9bab3de41930f32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PH\n",
      "C2\n",
      "C1\n",
      "PX\n",
      "FYVE\n",
      "BAR\n",
      "ENTH\n",
      "SH2\n",
      "SEC14\n",
      "START\n",
      "C2DIS\n",
      "GLA\n",
      "PLD\n",
      "PLA\n",
      "ANNEXIN\n",
      "\n",
      "> Adding the Sequence data in the dataset...\n"
     ]
    }
   ],
   "source": [
    "DATASET = builder.sequence.add_sequence_in_dataset(DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding info from uniprot protein sheet\n",
    "\n",
    "Uniprot page can have a lot of usefull data (taxon, origin crossreferences). First we will download all UNIPROT xml sheet for every `uniprotid` we have in our dataset and then we parse it to get the data we want.  \n",
    "For now the data we keep are \n",
    " - [x] `uniprot_id` (for matching)\n",
    " - [x] `uniprot_acc` (for matching)\n",
    " - [x] `location` in the cell\n",
    " - [x] `taxon` for species \n",
    " - [x] `CR:prositeID` Cross reference with PROSITE ID like `PS50003`\n",
    " - [x] `CR:prositeName` for prosite name (like `PH`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bda9278c6cf471a9e61c4a3576bb0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6287.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> 0 new files downloaded and 6287 will be reused.\n",
      "> Parsing uniprot files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff57270f3c3484e8697afb68496faa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6332.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "builder.sequence.download_uniprot_data(DATASET)\n",
    "DATASET = builder.sequence.add_info_from_uniprot(DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Cluster info \n",
    "\n",
    "To remove the redundancy we can use the \"Uniref\" cluster info. There are 3 differents cluster level:\n",
    "- `uniref50` at 50% of sequence idendity \n",
    "- `uniref90` at 90% of sequence idendity \n",
    "- `uniref100` at 100% of sequence idendity  \n",
    "\n",
    "Every sequences with more than `50`/`90`/`100`% of sequence identity are cluseters together.  \n",
    "More info can be found here https://www.uniprot.org/help/uniref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to fetch NF50\n",
      "  >done\n",
      "trying to fetch NF90\n",
      "  >done\n",
      "trying to fetch NF100\n",
      "  >done\n",
      "> mapping with dataset\n",
      "  >ok<  \n"
     ]
    }
   ],
   "source": [
    "DATASET = builder.sequence.add_cluster_info(DATASET)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Conservation\n",
    "\n",
    "Per-position conservation is important to have information on the importance of each amino acid in the protein. If a residue is conserved it means that it can have a critical role for the protein.  \n",
    "For now we will use only one formula to calculate the conservation: The **shannon entropy**.  \n",
    "Shanon Entropy(Shannon, 1948) is possibly the most sensitive tool to estimate the diversity of a system (http://imed.med.ucm.es/Tools/svs_help.html#ref).  \n",
    "For a multiple protein sequence alignment the Shannon entropy H for every position of the alignment is:  \n",
    "$$H=-\\sum_{i=1}^{M} P_i\\,log_2\\,P_i$$\n",
    "\n",
    " - Where $M$ is the number of different amino acids \n",
    " - $P_i$ is the 'propability' to have a specific amino acid on a column (number of occurences of this amino acid on $M$)\n",
    "\n",
    "From the github package by Felix Francis: https://github.com/ffrancis/Multiple-sequence-alignment-Shannon-s-entropy/blob/master/msa_shannon_entropy012915.py\n",
    "\n",
    "- H ranges from 0 (only one base/residue in present at that position) to 4.322 (all 20 residues are equally represented in that position).  \n",
    "- Typically, positions with H >2.0 are considerered variable, whereas those with H < 2 are consider conserved.  \n",
    "- Highly conserved positions are those with H <1.0 (Litwin and Jores, 1992). A minimum number of sequences is however required (~100) for H to describe the diversity of a protein family.  \n",
    "\n",
    "*Shannon, C. E. (1948) The mathematical theory of communication. The Bell system Technical Journal, 27, 379-423 & 623-656.*  \n",
    "*Litwin S., Jores R. (1992) Shannon Information as a Measure of Amino Acid Diversity. In: Perelson A.S., Weisbuch G. (eds) Theoretical and Experimental Insights into Immunology. NATO ASI Series (Series H: Cell Biology), vol 66. Springer, Berlin, Heidelberg*\n",
    "\n",
    "**We will use 2 Shannon entropy calculation** :\n",
    " - The regular one. All amino acids are considered are \"unique\". $Entropy_{M = 21} \\in [0,4.33 \\pm 0.01]$ estimated on 10000 repetitions of $N=10000$ random draw amoung 20 amino acids + gaps.\n",
    " - The H10 one. Amino acids are regroupes according their type (see http://thegrantlab.org/bio3d/reference/entropy.html), $Entropy_{M = 10}\\in [0,3.1 \\pm 0.01]$ estimated on 10000 repetitions of $N=10000$ random draw amoung 10 groups.\n",
    "   `Hydrophobic/Aliphatic [V,I,L,M], Aromatic [F,W,Y], Ser/Thr [S,T], Polar [N,Q], Positive [H,K,R], Negative [D,E], Tiny [A,G], Proline [P], Cysteine [C], and Gaps [-,X].`  \n",
    "    \n",
    "**NOTE** : The conservation is normalized between 0 and 1, 0 is not conserved, 1 is fully conserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'builder.Builder' from '/Users/thibault/OneDrive - University of Bergen/projects/peprmint/dev/pepermintdataset/notebooks/builder/Builder.py'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acbb98d70775494386d13c42df9847e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(builderEngine)\n",
    "builder = builderEngine.Builder(SETUP, recalculate = recalculation_widget.value, notebook = True)\n",
    "\n",
    "\n",
    "DATASET = builder.sequence.add_conservation(DATASET, gapcutoff=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## size optimisation\n",
    "Last thing to do before saving, is optimizing the dataset size. This will improve computational time (and disk storage of course üôÇ )\n",
    "One way to do it, it's to simplify the variables type. For example, change `str` to `category`\" will help decrease the space in memory a lot for variables like `domain` or `resdidue_name`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> datatypes optimisation\n",
      "Size BEFORE optimization 4668.52 MB\n",
      "Size AFTER optimization 2655.76 MB\n",
      "56.89% of the original size\n"
     ]
    }
   ],
   "source": [
    "DATASET = builder.optimize_size(DATASET)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicated residues\n",
    "\n",
    "I don't know why, but sometimes I have duplicated residues (the whole cathpdb is duplicated) so just in case, we remove the duplicates based on `'atom_number','atom_name','residue_name','residue_number','cathpdb','chain_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = DATASET.drop_duplicates(subset=['atom_number','atom_name','residue_name','residue_number','cathpdb','chain_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final saving\n",
    "\n",
    "**FINALY!** We have our final dataset !!! \n",
    "We save it into 2 versions: Full (with all PDB atoms) and light (only with `CA` and `CB` atoms).  \n",
    "‚û°Ô∏è  For now we don't need other atoms for analysis, just CA and CB are enough and it can make the analysis faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.save_checkpoint_dataset(DATASET,\"DATASET_peprmint_allatoms_d25\")\n",
    "builder.save_checkpoint_dataset(DATASET.query(\"atom_name in ['CA','CB']\"),\"DATASET_peprmint_d25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PX',\n",
       " 'PLD',\n",
       " 'C2DIS',\n",
       " 'SH2',\n",
       " 'ENTH',\n",
       " 'FYVE',\n",
       " 'C1',\n",
       " 'START',\n",
       " 'ANNEXIN',\n",
       " 'GLA',\n",
       " 'PLA',\n",
       " 'PH',\n",
       " 'SEC14',\n",
       " 'BAR',\n",
       " 'C2']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(DATASET.domain.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1unpA00' '2x18G00' '1eazA00' '2hthB00' '2dx5A00' '1rrpD00' '1shcA00'\n",
      " '3ml4A02' '2dyqA00' '3hk0A02' '4k81E02' '4k17B01' '4k17C01' '4k17A01'\n",
      " '4k17D01' '5efxA00' '3pvlA04' '1pmsA00' '2dtcB00' '3mpxA02' '2yf0A01'\n",
      " '2rgnB02' '3f0wA00' '2nmbA00' '1wj1A01' '4gzuB03' '1foeE02' '4gzuB02'\n",
      " '1v5uA00' '1wgqA00' '2d9yA00' '2yryA00' '2dkpA01' '2rovA00' '4tyzB00'\n",
      " '1v89A00' '2coaA01' '2d9zA01' '2ec1A00' '2m14A00' '2dn6A00' '2mfqA00'\n",
      " '1fhoA00' '1plsA00' '2ys1A00' '1wi1A01' '3f5rA00' '4khbD01' '1tqzA01'\n",
      " '2d9wA01' '1z87A01' '5gowB00' '2cofA00' '2rloA00' '1v88A00' '1v5pA01'\n",
      " '1v61A00' '2cocA01' '2iybD00' '1qc6A00' '1egxA00' '2dhkA01' '2m38A00'\n",
      " '1v5mA00' '2codA01' '1xr0B01' '2ys5A01' '1mkeA00' '1x1fA00' '2fjlA00'\n",
      " '1wg7A01' '2yt0A01' '2i5fA00' '1x05A00' '1x1gA00' '2p8vA00' '1ddvA00'\n",
      " '1i7aD00' '2qkmC00' '5dh9B00' '4l6eA00' '1k5dB00' '1xkeA00' '1ntyA02'\n",
      " '3aj4B00' '2dhiA00' '2d9vA01' '1i2hA00' '4f7uP00' '4f77e00' '1wvhA00'\n",
      " '2dkqA01' '2oqbA00' '1u5dD00' '1u5fA01' '3so6A00' '5c6rA00' '2da0A00'\n",
      " '1maiA00' '3bjiB02' '4ioyX02' '4khaA02' '4z2mB02' '1xodB00' '3syxA00'\n",
      " '2jp2A00' '3fehA02' '4k2pC01' '3a8pA01' '2v76D00' '1uefB00' '2dlwA00'\n",
      " '3voqA00' '1w1dA00' '4emoA00' '1ef1A03' '3u8zD03' '2i1kA03' '2d11B03'\n",
      " '3fm8C03' '2z0qA02' '2rsgA00' '2d9xA01' '3pp2B00' '2p0hA00' '4tknB03'\n",
      " '2cy4A00' '2j59N00' '2dhjA00' '3dxdA00' '1wguA01' '2yszA01' '1zc4D00'\n",
      " '4bbkA00' '2lkoA00' '5l81A00' '2ys3A00' '2ej8B00' '1x86G02' '3t06A02'\n",
      " '3odxB02' '1zvrA01' '5c16C01' '1faoA00' '4wsfA00' '1mphA00' '1wjmA00'\n",
      " '1droA00' '2dynA00' '3zvrA03' '2cayA00' '4chmA00' '2vszA02' '2gcjD02'\n",
      " '4ifsA02' '5c5bA02' '4h8sD02' '3tvvA02' '3to1B02' '3gyoA02' '1aqcA00'\n",
      " '1x11B00' '1x11A00' '4dbbA00' '3suzA01' '4d0nB02' '2lg1A02' '2y8fA00'\n",
      " '2crfA01' '2dx1A03' '3u7dA03' '3p7zA02' '5d3wB00' '2y7bA00' '3oanA00'\n",
      " '4nswB02' '4f7gA02' '3g9wA02' '2al6A03' '3rcpA00' '2kcjA00' '4a6kB00'\n",
      " '4chjA00' '1ntvA00' '1p3rC00' '1dbhA02' '2w2wB00' '5he2A05' '3au4A04'\n",
      " '3qijB03' '2he7A03' '1qqgA02' '4c0aA03' '1qqgA01' '1fgzA00' '1u29A00'\n",
      " '4kaxB00' '4ny0C03' '4ekuA03' '5kcvA01' '1p6sA00' '3tfmA02' '4kvgD02'\n",
      " '4gn1D02' '4y94A00' '2lulA00' '3d8eB00' '4mt7A02' '3ulcA01' '1rj2J02'\n",
      " '3qbvD02' '1uprA00' '4wj7C00' '3ml4C01' '3wyfB00' '1j0wB00' '4b6hB00'\n",
      " '2lydA00' '1q67B01' '3hw2B00' '4iapB01' '4dx8D00' '3tfmA01']\n"
     ]
    }
   ],
   "source": [
    "#Generate alignment file list for C2DIS domain (S95, because everything is just too slow, too much structure)\n",
    "\n",
    "def selectUniquePerCluster(df, cathCluster, Uniref, withAlignment=True, pdbreference=None,\n",
    "                               removeStrand=False):\n",
    "        \"\"\"\n",
    "        Return a datasert with only 1 data per choosed clusters.\n",
    "        \"\"\"\n",
    "\n",
    "        if cathCluster not in [\"S35\", \"S60\", \"S95\", \"S100\"]:\n",
    "            raise ValueError('CathCluster given not in [\"S35\",\"S60\",\"S95\",\"S100\"]')\n",
    "\n",
    "        if Uniref not in [\"uniref50\", \"uniref90\", \"uniref100\"]:\n",
    "            raise ValueError('CathCluster given not in [\"uniref50\",\"uniref90\",\"uniref100\"]')\n",
    "\n",
    "        if withAlignment:\n",
    "            df = df[~df.alignment_position.isnull()]\n",
    "\n",
    "        cathdf = df.query(\"data_type == 'cathpdb'\")\n",
    "        seqdf = df.query(\"data_type == 'prosite'\")\n",
    "\n",
    "        def selectUniqueCath(group):\n",
    "            uniqueNames = group.cathpdb.unique()\n",
    "            if pdbreference:\n",
    "                if pdbreference in uniqueNames:\n",
    "                    select = pdbreference\n",
    "                else:\n",
    "                    select = uniqueNames[0]\n",
    "            else:\n",
    "                select = uniqueNames[0]\n",
    "\n",
    "            # return group.query(\"cathpdb == @select\")\n",
    "            return select\n",
    "\n",
    "        def selectUniqueUniref(group, exclusion):\n",
    "            uniqueNames = group.uniprot_acc.unique()\n",
    "            select = uniqueNames[0]\n",
    "            # return group.query(\"uniprot_acc == @select\")\n",
    "            if select not in exclusion:\n",
    "                return select\n",
    "        dfReprCathNames = cathdf.groupby([\"domain\", cathCluster]).apply(selectUniqueCath).to_numpy()\n",
    "        print(dfReprCathNames)\n",
    "        excludeUniref = df.query(\"cathpdb in @dfReprCathNames\").uniprot_acc.unique()  # Structures are prior to sequences.\n",
    "        dfReprUnirefNames = seqdf.groupby([\"domain\", Uniref]).apply(selectUniqueUniref,\n",
    "                                                                    exclusion=excludeUniref).to_numpy()\n",
    "        dfReprCath = cathdf.query(\"cathpdb in @dfReprCathNames\")\n",
    "        dfReprUniref = seqdf.query(\"uniprot_acc in @dfReprUnirefNames\")\n",
    "\n",
    "        return (pd.concat([dfReprCath, dfReprUniref]))\n",
    "\n",
    "    \n",
    "c2dis = selectUniquePerCluster(DATASET.query(\"domain== 'PH'\"), 'S95', 'uniref90', withAlignment=False, pdbreference='2da0A00')\n",
    "#pdblist = c2dis.cathpdb.dropna().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['atom_number', 'atom_name', 'residue_name', 'chain_id',\n",
       "       'residue_number', 'x_coord', 'y_coord', 'z_coord', 'occupancy',\n",
       "       'b_factor', 'sec_struc', 'sec_struc_full', 'prot_block',\n",
       "       'sasa_rel_dssp', 'ASA_res_freesasa_florian', 'RSA_freesasa_florian',\n",
       "       'ASA_total_freesasa', 'ASA_mainchain_freesasa',\n",
       "       'ASA_sidechain_freesasa', 'RSA_sidechain_freesasa',\n",
       "       'RSA_total_freesasa_tien', 'RSA_sidechain_freesasa_tien',\n",
       "       'sec_struc_segment', 'pdb', 'domain', 'cathpdb', 'uniprot_acc',\n",
       "       'data_type', 'Experimental Method', 'convhull_vertex',\n",
       "       'co_insertable_neighbors', 'density', 'is_co_insertable',\n",
       "       'is_hydrophobic_protrusion', 'neighboursID', 'protrusion', 'LDCI',\n",
       "       'S35', 'S60', 'S95', 'S100', 'S100Count', 'resolution', 'uniprot_id',\n",
       "       'origin', 'residue_index', 'alignment_position', 'prositeName',\n",
       "       'prositeID', 'ali_range', 'location', 'CR:prositeID', 'taxon',\n",
       "       'CR:prositeName', 'uniref50', 'uniref90', 'uniref100', 'shannon',\n",
       "       'shannonH10', 'type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET.query(\"atom_name == 'CA' and domain =='PH'\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = DATASET.query(\"atom_name == 'CA' and domain =='PH' and data_type == 'cathpdb'\")[[\"ASA_res_freesasa_florian\",\"RSA_freesasa_florian\",\"ASA_total_freesasa\",\"ASA_mainchain_freesasa\",\"ASA_sidechain_freesasa\",\"RSA_sidechain_freesasa\",\"RSA_total_freesasa_tien\",\"RSA_sidechain_freesasa_tien\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f562939bf91b46b655e3a917bf756908169d96bc255520a8d7ed621cd1fd7da"
  },
  "kernelspec": {
   "display_name": "Python [conda env:peprmint]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
